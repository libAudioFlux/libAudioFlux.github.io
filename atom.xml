<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>audioFlux</title>
  
  <subtitle>Blog</subtitle>
  <link href="http://libaudioflux.github.io/atom.xml" rel="self"/>
  
  <link href="http://libaudioflux.github.io/"/>
  <updated>2023-04-25T11:55:58.750Z</updated>
  <id>http://libaudioflux.github.io/</id>
  
  <author>
    <name>audioFlux</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Benchmark for Audio Libraries: Audioflux, TorchAudio, Librosa, Essentia, etc</title>
    <link href="http://libaudioflux.github.io/post/benchmark/"/>
    <id>http://libaudioflux.github.io/post/benchmark/</id>
    <published>2023-04-25T23:45:54.000Z</published>
    <updated>2023-04-25T11:55:58.750Z</updated>
    
    <content type="html"><![CDATA[<h2 id="audioFlux"><a href="#audioFlux" class="headerlink" title="audioFlux"></a>audioFlux</h2><blockquote><p><a href="https://github.com/libAudioFlux/audioFlux"><strong><code>audioFlux</code></strong></a> is a library implemented in C and Python, which provides systematic, comprehensive and multi-dimensional feature extraction and combination in the audio field. In combination with various deep learning network models, it carries out business research and development in the audio field.</p></blockquote><h2 id="Introduce"><a href="#Introduce" class="headerlink" title="Introduce"></a>Introduce</h2><p>In the field of deep learning for audio, the mel spectrogram is the most commonly used audio feature. The performance of mel spectrogram features can be benchmarked and compared using audio feature extraction libraries such as the following:</p><span id="more"></span><table><thead><tr><th>Library</th><th>Language</th><th>Version</th><th>About</th></tr></thead><tbody><tr><td><a href="https://github.com/libAudioFlux/audioFlux">audioFlux</a></td><td>C&#x2F;Python</td><td>0.1.5</td><td>A library for audio and music analysis, feature extraction</td></tr><tr><td><a href="https://github.com/pytorch/audio">torchaudio</a></td><td>Python</td><td>0.11.0</td><td>Data manipulation and transformation for audio signal processing, powered by PyTorch</td></tr><tr><td><a href="https://github.com/librosa/librosa">librosa</a></td><td>Python</td><td>0.10.0</td><td>C++ library for audio and music analysis, description and synthesis, including Python bindings</td></tr><tr><td><a href="https://github.com/MTG/essentia">essentia</a></td><td>C++&#x2F;Python</td><td>2.0.1</td><td>Python library for audio and music analysis</td></tr></tbody></table><ul><li>audioFlux: developed in C with a Python wrapper, it has different bridging processes for different platforms, and supports <strong>OpenBLAS</strong>, <strong>MKL</strong>, etc.</li><li>torchaudio: developed in PyTorch, which is optimized for CPUs and uses <strong>MKL</strong> as its backend. This evaluation does not include the GPU version of PyTorch.</li><li>librosa: developed purely in Python, mainly based on <strong>NumPy</strong> and <strong>SciPy</strong>, with NumPy using <strong>OpenBLAS</strong> as its backend.</li><li>essentia: developed in C++ with a Python wrapper, it uses <strong>Eigen</strong> and <strong>FFTW</strong> as its backend.</li></ul><p>There are many factors that can affect the performance evaluation results, including CPU architecture, operating system, compilation system, selection of basic linear algebra libraries, and usage of project APIs, all of which can have a certain impact on the evaluation results.</p><p>For the most common mel features in the audio field, the major performance bottlenecks are FFT computation, matrix computation, and multi-threaded parallel processing, while minor bottlenecks include algorithmic business implementation and Python packaging.</p><ul><li>Regarding FFT computation, librosa uses SciPy’s fftpack for accelerated FFT computation, which is slower than FFTW3, MKL, and Accelerate.    </li><li>Regarding matrix computation, MKL is faster than OpenBLAS, while OpenBLAS is faster than Eigen.    </li><li>Regarding multi-threaded parallel processing, it depends on whether each project has support for it.</li></ul><h2 id="Scripts"><a href="#Scripts" class="headerlink" title="Scripts"></a>Scripts</h2><p>If you want to compare and test multiple libraries, you can use:  </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">python run_benchmark.py -p audioflux,torchaudio,librosa -r 1000 -er 10  -t 1,5,10,100,500,1000,2000,3000</span></span><br></pre></td></tr></table></figure><ul><li>-p:  The library name, list</li><li>-r:  The number of sample data, number</li><li>-er: The number of <code>run_xxx.py</code>  calls, number</li><li>-t:  The time of each sample data, list</li></ul><p>If you want to test a single library, you can use:    </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">python run_audioflux.py -r 1000 -t 1,5,10,100,500,1000,2000,3000</span></span><br></pre></td></tr></table></figure><p>If you want to see more usage instructions, you can execute <code>python run_xxx.py --help</code></p><h3 id="Notice"><a href="#Notice" class="headerlink" title="Notice"></a>Notice</h3><p>In the field of audio, libraries related to audio feature extraction have their own functional characteristics and provide different types of features. This evaluation does not aim to test all the performance comparisons of their feature extraction in detail. However, as the mel spectrum is one of the most important and fundamental features, all of these libraries support it.</p><p>There are many factors that can affect the performance evaluation results, such as CPU architecture, operating system, compilation system, choice of basic linear algebra library, and the usage of project APIs, which will have a certain impact on the evaluation results. In order to be as fair as possible and to better reflect actual business needs, the following conditions are based on in this evaluation:</p><ol><li>macOS&#x2F;Linux operating system, three types of CPUs: Intel&#x2F;AMD&#x2F;M1.</li><li>The libraries use the latest official release version or the latest official source code compiled with high performance support, and the fastest one is selected.</li><li>In terms of API usage, the official standards are followed, and “warming up” is used for each corresponding method of the libraries (the first execution time is not counted), and the execution time of the initialization is not counted.</li><li>In terms of data length, various actual business considerations are taken into account when selecting the test data.</li></ol><blockquote><p>When the data is short, the first execution time of most libraries may be relatively slow. To reflect actual business needs and to be fair, this first execution time is not counted. If the library API design provides initialization functions, they will be created and repeatedly called in actual business scenarios, and the initialization execution time is also not counted. </p></blockquote><h3 id="Warn"><a href="#Warn" class="headerlink" title="Warn"></a>Warn</h3><blockquote><p>⚠️ When using Python scientific computing related libraries such as Conda, PyTorch, TensorFlow, XGBoost, LightGBM, etc., almost all of them use Intel Math Kernel Library (MKL). MKL uses OpenMP for parallel acceleration, but only one instance of OpenMP can exist in the same process. When these libraries are used together, it is best to link all libraries to the same location of libomp, otherwise an error will occur. Modifying the environment variables according to the prompt may result in slower program execution and unreliable results. Relevant tools can be used to rewrite the libomp linking path of the related libraries.</p></blockquote><h2 id="Performance"><a href="#Performance" class="headerlink" title="Performance"></a>Performance</h2><h3 id="Base-benchmark"><a href="#Base-benchmark" class="headerlink" title="Base benchmark"></a>Base benchmark</h3><p>Use audioFlux&#x2F;torchaudio&#x2F;librosa script, for AMD&#x2F;Intel&#x2F;M1 CPUs and Linux&#x2F;macOS operating system. </p><p>The time required to calculate the mel-spectrogram for 1000 sample data according to a TimeStep of 1&#x2F;5&#x2F;10&#x2F;100&#x2F;500&#x2F;1000&#x2F;2000&#x2F;3000. Where fft_len&#x3D;2048, slide_len&#x3D;512, sampling_rate&#x3D;32000.   </p><h4 id="Linux-AMD"><a href="#Linux-AMD" class="headerlink" title="Linux - AMD"></a>Linux - AMD</h4><pre><code>- OS: Ubuntu 20.04.4 LTS- CPU: AMD Ryzen Threadripper 3970X 32-Core Processor</code></pre><p><img src="/post/benchmark/linux_amd_1.png"></p><table><thead><tr><th>TimeStep</th><th>audioflux</th><th>torchaudio</th><th>librosa</th></tr></thead><tbody><tr><td>1</td><td>0.04294s</td><td>0.07707s</td><td>2.41958s</td></tr><tr><td>5</td><td>0.14878s</td><td>1.05589s</td><td>3.52610s</td></tr><tr><td>10</td><td>0.18374s</td><td>0.83975s</td><td>3.46499s</td></tr><tr><td>100</td><td>0.67030s</td><td>0.61876s</td><td>6.63217s</td></tr><tr><td>500</td><td>0.94893s</td><td>1.29189s</td><td>16.45968s</td></tr><tr><td>1000</td><td>1.43854s</td><td>2.23126s</td><td>27.78358s</td></tr><tr><td>2000</td><td>3.08714s</td><td>4.10869s</td><td>45.12714s</td></tr><tr><td>3000</td><td>4.90343s</td><td>5.86299s</td><td>51.62876s</td></tr></tbody></table><h4 id="Linux-Intel"><a href="#Linux-Intel" class="headerlink" title="Linux - Intel"></a>Linux - Intel</h4><pre><code>- OS: Ubuntu 20.04.4 LTS- CPU: Intel(R) Core(TM) i7-6850K CPU @ 3.60GHz</code></pre><p><img src="/post/benchmark/linux_intel_1.png"></p><table><thead><tr><th>TimeStep</th><th>audioflux</th><th>torchaudio</th><th>librosa</th></tr></thead><tbody><tr><td>1</td><td>0.08106s</td><td>0.11043s</td><td>5.51295s</td></tr><tr><td>5</td><td>0.11654s</td><td>0.16005s</td><td>5.77631s</td></tr><tr><td>10</td><td>0.29173s</td><td>0.15352s</td><td>6.13656s</td></tr><tr><td>100</td><td>1.18150s</td><td>0.39958s</td><td>10.61641s</td></tr><tr><td>500</td><td>2.23883s</td><td>1.58323s</td><td>28.99823s</td></tr><tr><td>1000</td><td>4.42723s</td><td>3.98896s</td><td>51.97518s</td></tr><tr><td>2000</td><td>8.73121s</td><td>8.28444s</td><td>61.13923s</td></tr><tr><td>3000</td><td>13.07378s</td><td>12.14323s</td><td>70.06395s</td></tr></tbody></table><h4 id="macOS-Intel"><a href="#macOS-Intel" class="headerlink" title="macOS - Intel"></a>macOS - Intel</h4><pre><code>- OS: 12.6.1 (21G217)- CPU: 3.8GHz 8‑core 10th-generation Intel Core i7, Turbo Boost up to 5.0GHz</code></pre><p><img src="/post/benchmark/mac_x86_1.png"></p><table><thead><tr><th>TimeStep</th><th>audioflux</th><th>torchaudio</th><th>librosa</th></tr></thead><tbody><tr><td>1</td><td>0.07605s</td><td>0.06451s</td><td>1.70139s</td></tr><tr><td>5</td><td>0.14946s</td><td>0.08464s</td><td>1.86964s</td></tr><tr><td>10</td><td>0.16641s</td><td>0.10762s</td><td>2.00865s</td></tr><tr><td>100</td><td>0.46902s</td><td>0.83551s</td><td>3.28890s</td></tr><tr><td>500</td><td>1.08860s</td><td>5.05824s</td><td>8.98265s</td></tr><tr><td>1000</td><td>2.64029s</td><td>9.78269s</td><td>18.24391s</td></tr><tr><td>2000</td><td>5.40025s</td><td>15.08991s</td><td>33.68184s</td></tr><tr><td>3000</td><td>7.92596s</td><td>24.84823s</td><td>47.35941s</td></tr></tbody></table><h4 id="macOS-M1"><a href="#macOS-M1" class="headerlink" title="macOS - M1"></a>macOS - M1</h4><pre><code>- OS: 12.4 (21F79)- CPU: Apple M1</code></pre><p><img src="/post/benchmark/mac_arm_1.png"></p><table><thead><tr><th>TimeStep</th><th>audioflux</th><th>torchaudio</th><th>librosa</th></tr></thead><tbody><tr><td>1</td><td>0.06110s</td><td>0.06874s</td><td>2.22518s</td></tr><tr><td>5</td><td>0.23444s</td><td>0.07922s</td><td>2.55907s</td></tr><tr><td>10</td><td>0.20691s</td><td>0.11090s</td><td>2.71813s</td></tr><tr><td>100</td><td>0.68694s</td><td>0.63625s</td><td>4.74433s</td></tr><tr><td>500</td><td>1.47420s</td><td>3.37597s</td><td>13.83887s</td></tr><tr><td>1000</td><td>3.00926s</td><td>6.76275s</td><td>25.24646s</td></tr><tr><td>2000</td><td>5.99781s</td><td>12.69573s</td><td>47.84029s</td></tr><tr><td>3000</td><td>8.76306s</td><td>19.03391s</td><td>69.40428s</td></tr></tbody></table><h3 id="Summarize"><a href="#Summarize" class="headerlink" title="Summarize"></a>Summarize</h3><p>In summary, from the performance comparison results of the three libraries, librosa takes the most time, which is also in line with common sense.<br>On linux&#x2F;amd processors, audioflux is slightly faster than torchaudio, but slightly slower on linux&#x2F;intel.<br>On the macOS system, for large-size sample data, audioflux is faster than torchaudio, and intel is more obvious than m1; for small-size sample data, torchaudio is faster than audioflux.   </p><p>⚠️ Although the development of benchmark is attempted to be as objective and fair as possible, every benchmarks have their drawbacks, and are limited to particular testing procedures, datasets and platforms. And also, this benchmark does not compare additional features that a library may support, or other APIs,  cross-platform, etc. We encourage users to benchmarks with their own data sets and platforms.  </p><h3 id="Other-Test"><a href="#Other-Test" class="headerlink" title="Other Test"></a>Other Test</h3><h4 id="Server-Performance"><a href="#Server-Performance" class="headerlink" title="Server Performance"></a>Server Performance</h4><p>Each sample data is 128ms(sampling rate: 32000, data length: 4096).</p><p>The total time spent on extracting features for 1000 sample data.</p><pre><code>- OS: Ubuntu 20.04.4 LTS- CPU: AMD Ryzen Threadripper 3970X 32-Core Processor</code></pre><table><thead><tr><th>Package</th><th><a href="https://github.com/libAudioFlux/audioFlux">audioFlux</a></th><th><a href="https://github.com/librosa/librosa">librosa</a></th><th><a href="https://github.com/tyiannak/pyAudioAnalysis">pyAudioAnalysis</a></th><th><a href="https://github.com/jameslyons/python_speech_features">python_speech_features</a></th></tr></thead><tbody><tr><td>Mel</td><td>0.777s</td><td>2.967s</td><td>–</td><td>–</td></tr><tr><td>MFCC</td><td>0.797s</td><td>2.963s</td><td>0.805s</td><td>2.150s</td></tr><tr><td>CQT</td><td>5.743s</td><td>21.477s</td><td>–</td><td>–</td></tr><tr><td>Chroma</td><td>0.155s</td><td>2.174s</td><td>1.287s</td><td>–</td></tr></tbody></table><h4 id="Mobile-Performance"><a href="#Mobile-Performance" class="headerlink" title="Mobile Performance"></a>Mobile Performance</h4><p>For 128ms audio data per frame(sampling rate: 32000, data length: 4096).</p><p>The time spent on extracting features for 1 frame data.</p><table><thead><tr><th>Mobile</th><th>iPhone 13 Pro</th><th>iPhone X</th><th>Honor V40</th><th>OPPO Reno4 SE 5G</th></tr></thead><tbody><tr><td>Mel</td><td>0.249ms</td><td>0.359ms</td><td>0.313ms</td><td>0.891ms</td></tr><tr><td>MFCC</td><td>0.249ms</td><td>0.361ms</td><td>0.315ms</td><td>1.116ms</td></tr><tr><td>CQT</td><td>0.350ms</td><td>0.609ms</td><td>0.786ms</td><td>1.779ms</td></tr><tr><td>Chroma</td><td>0.354ms</td><td>0.615ms</td><td>0.803ms</td><td>1.775ms</td></tr></tbody></table>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;audioFlux&quot;&gt;&lt;a href=&quot;#audioFlux&quot; class=&quot;headerlink&quot; title=&quot;audioFlux&quot;&gt;&lt;/a&gt;audioFlux&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/libAudioFlux/audioFlux&quot;&gt;&lt;strong&gt;&lt;code&gt;audioFlux&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt; is a library implemented in C and Python, which provides systematic, comprehensive and multi-dimensional feature extraction and combination in the audio field. In combination with various deep learning network models, it carries out business research and development in the audio field.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;Introduce&quot;&gt;&lt;a href=&quot;#Introduce&quot; class=&quot;headerlink&quot; title=&quot;Introduce&quot;&gt;&lt;/a&gt;Introduce&lt;/h2&gt;&lt;p&gt;In the field of deep learning for audio, the mel spectrogram is the most commonly used audio feature. The performance of mel spectrogram features can be benchmarked and compared using audio feature extraction libraries such as the following:&lt;/p&gt;</summary>
    
    
    
    
    <category term="Benchmark" scheme="http://libaudioflux.github.io/tags/Benchmark/"/>
    
  </entry>
  
  <entry>
    <title>Introducing audioFlux: a deep learning tool for audio feature extraction</title>
    <link href="http://libaudioflux.github.io/post/audio-features/"/>
    <id>http://libaudioflux.github.io/post/audio-features/</id>
    <published>2023-03-08T20:34:55.000Z</published>
    <updated>2023-04-25T11:55:58.722Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/post/audio-features/logo.png"></p><h3 id="Introduce"><a href="#Introduce" class="headerlink" title="Introduce"></a>Introduce</h3><p><strong><code>audioFlux</code></strong>  is a library implemented in C and Python, which provides systematic, comprehensive and multi-dimensional feature extraction and combination in the audio field. In combination with various deep learning network models, it carries out business research and development in the audio field. Next, it briefly describes its relevant functions from six aspects: time-frequency transform, reassign spectrogram, cepstrum coefficient, deconvolution, spectral features, and music information retrieval.</p><h3 id="Time-Frequency-Transform"><a href="#Time-Frequency-Transform" class="headerlink" title="Time-Frequency Transform"></a>Time-Frequency Transform</h3><p><strong><code>audioFlux</code></strong>  in the field of time-frequency analysis, the following general transform algorithms (supporting all subsequent frequency scale types) are included:</p><span id="more"></span><p><strong>BFT</strong> — Based Fourier Transform. The equivalent short-time Fourier transform (STFT) is generally based on this to realize the well-known mel spectrum. The Fourier transform with overlapping translation and window in the time domain is generally 1&#x2F;4 of the length of the overlapping transform. The Gaussian window is also called Gabor transform. The length of the window function can be adjusted to facilitate the simulation of time-frequency analysis characteristics. In addition to providing standard mel&#x2F;bar&#x2F;erb and other spectra, the BFT algorithm also supports the complex spectrum of mel scale types, It also supports the rearrangement of mel equiscale spectrum.</p><p><strong>NSGT</strong> — Non-Stationary Gabor Transform. Similar to STFT (short time Fourier transform) with Gaussian window, the difference is that the window function length and t establish a non-stationary relationship. Relative to STFT, it can achieve better analysis of non-stationary states in steady-state signals. The better onset endpoint detection effect is often based on such spectrum calculation, and can be used as an efficient way to achieve CQT. In this algorithm, the octave frequency scale type of NSGT transform is the efficient implementation of CQT.</p><p><strong>CWT</strong> — Continuous Wavelet Transform. Multi-resolution time-frequency analysis: mathematically, the base of Fourier transform is infinite sin&#x2F;cos function, while the base of wavelet transform is finite and very small wave function, The general expression of wave function is <img src="/post/audio-features/f1.png"> among them, a determines the scaling scale of frequency domain, b time translation scale, and establishes adaptive time-frequency analysis. Compared with the fixed time-frequency resolution of STFT, it has the characteristics of high frequency resolution in low frequency band and high time resolution in high frequency band, which is very suitable for non-stationary signal analysis, and supports the commonly used Morse, Morlet, Bump, Pual, Meyer and other wave functions.</p><p><strong>PWT</strong> — Pseudo Wavelet Transform. Based on the similar effect of wavelet transform realized by Fourier transform, the efficient algorithm of CWT calculates filterBank and does dot operation for frequency domain wavelet function and frequency domain data. If the wavelet function is regarded as a special window function, it can produce CWT-like effect, namely pseudo-wavelet transform. If the window function establishes parameters of frequency band (not yet implemented in the library), it can be equivalent to wavelet transform.</p><p>The above conversion function supports all the following frequency scale types:</p><ul><li><strong>Linear</strong> — Standard linear frequency band spectrum, half of the result of short-time Fourier transform is linear scale spectrum, and the scale is sampling rate&#x2F;sampling sample, which is the minimum frequency resolution.</li><li><strong>Linsapce</strong> — The spectrum of the user-defined frequency band can be larger than the linear frequency band, which is equivalent to the time domain downsampling.</li><li><strong>Mel</strong> — Mel scale spectrogram, one of the most commonly used types of spectrogram in audio, is based on the characteristics of low frequency sensitivity and high frequency insensitivity of human hearing, and the log-like compressed linear scale.</li><li><strong>Bark</strong> — Bark scale spectrum diagram is more suitable for human hearing than Mel scale.</li><li><strong>ERB</strong> — Equivalent rectangular bandwidth scale spectrum, which is more consistent with human hearing than Barker scale.</li><li><strong>Octave</strong> — Octave scale spectrum.</li><li><strong>Log</strong> — Logarithmic scale spectrum.</li></ul><p>The following is a simple comparison diagram of different frequency scales under BFT transform.<br><img src="/post/audio-features/spec1.png"></p><p>The following is a simple comparison diagram of different CWT wave functions.<br><img src="/post/audio-features/spec2.png"></p><p>The following algorithms can be used as independent transform (multiple frequency scale types are not supported):</p><ul><li><strong>CQT</strong> — Constant Q transform, which is a transform with constant frequency band ratio, is often used in music. Chroma features are often calculated based on this to analyze harmony.</li><li><strong>VQT</strong> — Variable Q transform.</li><li><strong>ST</strong> — S transform&#x2F;Stockwell transform, similar to wavelet transform, is an extreme special case of wavelet transform thought, which can be used to detect and analyze some extreme abrupt signals, such as earthquakes, tsunamis, etc. Compared with NSGT, it also adds Gaussian windows, but establishes a scaling relationship between frequency and time.</li><li><strong>FST</strong> — Fast S transform, discrete base 2 implementation of S transform.</li><li><strong>DWT</strong> — Discrete wavelet transform, compared with CWT, frequency based on 2 transform.</li><li><strong>WPT</strong> — Wavelet packet transform, also known as wavelet packet decomposition, can perform detailed and approximate decomposition of signals. It is a way of signal separation and synthesis, and can be used for noise reduction, modal structure analysis and other businesses.</li><li><strong>SWT</strong> — Stationary wavelet transform, similar to wavelet packet transform, the decomposed signal is the same length as the original signal.</li></ul><p>The following is a simple comparison of different scales under CQT and NSGT transform.<br><img src="/post/audio-features/spec3.png"></p><blockquote><p>NSGT-Octave is clearer and more focused than CQT</p></blockquote><p>Chroma is a higher-level feature based on spectrum, belonging to the category of musical scale system. The scale for non-musical tones is worse than that for musical tones. At present, it supports the spectrum types of Chroma features:</p><ul><li>CQT</li><li>BFT-linear</li><li>BFT-octave</li></ul><p>Here is a simple comparison of Chroma.<br><img src="/post/audio-features/spec4.png"></p><blockquote><p>The spectrum with different frequency scales has their own application value. For some business situations, these spectrum maps with different scales can be combined to form a large feature set to participate in network training.</p><p>The spectrum diagram is called amplitude spectrum, power spectrum, logarithmic spectrum&#x2F;dB spectrum according to the type of value abs, square, log and other nonlinear operations. The logarithmic spectrum is generally used more in deep learning.</p></blockquote><h3 id="Reassign-Spectrogram"><a href="#Reassign-Spectrogram" class="headerlink" title="Reassign Spectrogram"></a>Reassign Spectrogram</h3><p>The synchrosqueezing or reassignment is a technique for sharpening a time-frequency representation, <strong><code>audioFlux</code></strong> includes the following algorithms:</p><ul><li>reassign — reassign transform for STFT.</li><li>synsq — reassign data use CWT data.</li><li>wsst — reassign transform for CWT.</li></ul><p>Below is a spectrum diagram and the effect diagram after corresponding rearrangement.<br><img src="/post/audio-features/spec5.png"></p><blockquote><p>The effect after reassign is better than that before reassign.</p><p>Since the reassign effect is so good, can you rearrange it several times based on the last result? How does this work? <strong><code>audioFlux</code></strong> reassign related algorithms provide multiple reassign mechanisms, and the specific effects can be compared by referring to the document.</p></blockquote><h3 id="Cepstrum-Coefficient"><a href="#Cepstrum-Coefficient" class="headerlink" title="Cepstrum Coefficient"></a>Cepstrum Coefficient</h3><p>Similar to mfcc (Mel frequency cepstrum coefficient) for mel spectrum, this feature service belongs to pitch removal, which is a feature reflecting the physical structure of pronunciation. It is typically used for voice recognition related services, and can be used for different instrument classification, structure refinement and other business model training.</p><p>The entire <strong><code>audioFlux</code></strong> in the project spectrum system, except mfcc and corresponding delta&#x2F;deltaDelta, all types of spectrum cepstrum coefficients, namely xxcc, are supported:</p><ul><li>lfcc</li><li>gtcc</li><li>bfcc</li><li>cqcc</li><li>……</li></ul><p>The cepstrum coefficients of different spectrum types represent the pitch correlation of different spectrum types, and have their own application values. For example, the paper of gtcc reflects that the phoneme effect in speech recognition business is better than that of mfcc, and the classification and structure refinement business of cqcc for musical instruments are far better than that of mfcc.</p><p>The following is a comparison chart of different spectral cepstrum coefficients for guitar music and audio.<br><img src="/post/audio-features/spec6.png"></p><blockquote><p>In the initial stage of guitar music, cqcc performs best, and in the subsequent stage of continuous stability, gtcc performs better.</p></blockquote><h3 id="Deconvolution"><a href="#Deconvolution" class="headerlink" title="Deconvolution"></a>Deconvolution</h3><p>In mathematics, deconvolution is the inverse operation of convolution, which can be used as an algorithm for signal decomposition. For the spectrum, the two decomposed data can be represented as the formant spectrum and the pitch spectrum. Compared with mfcc, formant is a more general physical structural feature of pronunciation.</p><p>In <strong><code>audioFlux</code></strong> , the deconvolution operation of all types of spectrum is supported, and its value lies in that for pitch related services, the model inference can be more accurate after the formant interference is removed; For services with structure-related features, the pitch can be removed to avoid the interference of model training.</p><p>The following is the deconvolution effect of mel spectrum for guitar 880hz audio.<br><img src="/post/audio-features/spec7.png"></p><blockquote><p>You can see the obvious separation effect between the formant part（timbre） and the pitch part of the guitar.</p></blockquote><h3 id="Spectral-Feature"><a href="#Spectral-Feature" class="headerlink" title="Spectral Feature"></a>Spectral Feature</h3><p>In <strong><code>audioFlux</code></strong> , there are dozens of spectral-related features, including those based on timbre correlation, statistical correlation, spectral flux correlation, singular value correlation, and so on.</p><p>For example:</p><ul><li>flatness</li><li>skewness</li><li>crest</li><li>slop</li><li>rolloff</li><li>……</li><li>centroid</li><li>spread</li><li>kurtosis</li><li>……</li><li>flux</li><li>hfc</li><li>mkl</li><li>……</li><li>……</li></ul><p>For all spectral features provided by <strong><code>audioFlux</code></strong> , more specific functional descriptions, examples, formulas, etc., please refer to <a href="http://audioflux.top/">official documents</a>.</p><p>The following is a comparison of some spectral feature.<br><img src="/post/audio-features/spec8.png"></p><h4 id="Music-Information-Retrieval"><a href="#Music-Information-Retrieval" class="headerlink" title="Music Information Retrieval"></a>Music Information Retrieval</h4><p><strong><code>audioFlux</code></strong>  provides mir-related fields such as pitch estimation, onset detection, hpss (harmonic percussion separation) and other related business algorithms.</p><p>The pitch estimation is based on YIN, STFT and other related algorithms. The following is the detection effect picture for a vocal practice pitch.<br><img src="/post/audio-features/spec9.png"></p><blockquote><p>Red is the actual reference pitch, and blue is the estimated pitch.</p></blockquote><p>The onset detection includes algorithms based on spectrum flux, novelty, and so on. The following is an endpoint detection rendering of a guitar sweep accompaniment.<br><img src="/post/audio-features/spec10.png"></p><blockquote><p>The red dotted line in the middle time domain image is superimposed by the endpoint detection position.</p></blockquote><p>HPSS includes median filtering, non-negative matrix decomposition (NMF) and other algorithms. The following is a separation effect that includes guitar playing and metronome audio. The upper part of the time domain effect, and the lower part corresponds to the frequency domain effect.<br><img src="/post/audio-features/spec11.png"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;/post/audio-features/logo.png&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;Introduce&quot;&gt;&lt;a href=&quot;#Introduce&quot; class=&quot;headerlink&quot; title=&quot;Introduce&quot;&gt;&lt;/a&gt;Introduce&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;&lt;code&gt;audioFlux&lt;/code&gt;&lt;/strong&gt;  is a library implemented in C and Python, which provides systematic, comprehensive and multi-dimensional feature extraction and combination in the audio field. In combination with various deep learning network models, it carries out business research and development in the audio field. Next, it briefly describes its relevant functions from six aspects: time-frequency transform, reassign spectrogram, cepstrum coefficient, deconvolution, spectral features, and music information retrieval.&lt;/p&gt;
&lt;h3 id=&quot;Time-Frequency-Transform&quot;&gt;&lt;a href=&quot;#Time-Frequency-Transform&quot; class=&quot;headerlink&quot; title=&quot;Time-Frequency Transform&quot;&gt;&lt;/a&gt;Time-Frequency Transform&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;&lt;code&gt;audioFlux&lt;/code&gt;&lt;/strong&gt;  in the field of time-frequency analysis, the following general transform algorithms (supporting all subsequent frequency scale types) are included:&lt;/p&gt;</summary>
    
    
    
    
  </entry>
  
</feed>
